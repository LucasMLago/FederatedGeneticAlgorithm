{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f00410a5",
   "metadata": {},
   "source": [
    "# MLP with GA hyperparameter tuning\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c7634b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix, roc_curve\n",
    "\n",
    "import multiprocessing\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from config import config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933b2ba9",
   "metadata": {},
   "source": [
    "## Basic Config\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878eaad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(config.SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7d1f9d",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfdcca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations to apply to the images\n",
    "# Convert to pytorch tensor and normalize\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # Normalize for 3 channel image \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a855c685",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root=\"../data\",\n",
    "    train=True,\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "len(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643b44ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root=\"../data\",\n",
    "    train=False,\n",
    "    transform=transform,\n",
    "    download=True    \n",
    ")\n",
    "len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdc25ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image shape and classes\n",
    "img, label = trainset[0]\n",
    "\n",
    "print(img.shape, label)\n",
    "print(trainset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6999ec6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataloaders(trainset, testset, batch_size: int, seed: int) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    train_size = int(0.8 * len(trainset))\n",
    "    val_size = len(trainset) - train_size\n",
    "    train_subset, val_subset = random_split(trainset, [train_size, val_size], generator=torch.Generator().manual_seed(seed))\n",
    "    \n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2, generator=torch.Generator().manual_seed(seed))\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=2, generator=torch.Generator().manual_seed(seed))\n",
    "    test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2, generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a727232a",
   "metadata": {},
   "source": [
    "## MLP\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dad4cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int, dropout_p: float, hidden_layers, activation_functions: list):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # Input layer\n",
    "        self.layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        self.layers.append(activation_functions[0]())\n",
    "        self.layers.append(nn.Dropout(dropout_p))\n",
    "\n",
    "        # Hidden layers\n",
    "        for i in range(hidden_layers - 1):\n",
    "            self.layers.append(nn.Linear(hidden_dim // (2 ** i), hidden_dim // (2 ** (i + 1))))\n",
    "            self.layers.append(activation_functions[i + 1]())\n",
    "            self.layers.append(nn.Dropout(dropout_p))\n",
    "        \n",
    "        # Output layer\n",
    "        self.layers.append(nn.Linear(hidden_dim // (2 ** (hidden_layers - 1)), output_dim))\n",
    "    \n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def get_params(self) -> Tuple:\n",
    "        return tuple(layer for layer in self.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a295564",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32 * 2, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32 * 2, out_channels=64 * 2, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64 * 2, out_channels=64 * 2, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=64 * 2, out_channels=128 * 2, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=128 * 2, out_channels=128 * 2, kernel_size=3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(in_channels=128 * 2, out_channels=128 * 2, kernel_size=3, padding=1)\n",
    "        self.conv7 = nn.Conv2d(in_channels=128 * 2, out_channels=256 * 2, kernel_size=3, padding=1)\n",
    "        self.conv8 = nn.Conv2d(in_channels=256 * 2, out_channels=256 * 2, kernel_size=3, padding=1)\n",
    "        self.conv9 = nn.Conv2d(in_channels=256 * 2, out_channels=256 * 2, kernel_size=3, padding=1)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(32 * 2)\n",
    "        self.bn2 = nn.BatchNorm2d(128 * 2)\n",
    "        self.bn3 = nn.BatchNorm2d(256 * 2)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        self.fc1 = nn.Linear(4096 * 2, 4096 * 2)\n",
    "        self.fc2 = nn.Linear(4096 * 2, 2048 * 2)\n",
    "        self.fc3 = nn.Linear(2048 * 2, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.relu(self.bn2(self.conv4(x)))\n",
    "        x = self.relu(self.conv5(x))\n",
    "        x = self.relu(self.conv6(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.relu(self.bn3(self.conv7(x)))\n",
    "        x = self.relu(self.conv8(x))\n",
    "        x = self.relu(self.conv9(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06696f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN().to(config.DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9bbd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(logits: torch.Tensor, labels: torch.Tensor) -> dict:\n",
    "    predicted = torch.argmax(logits, dim=1).cpu()\n",
    "    accuracy = (predicted.eq(labels).sum().item()) / labels.shape[0]\n",
    "    return {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9954995a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, loader: DataLoader, criterion) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Evaluate the model on the given DataLoader. Returns the average loss and accuracy.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss, total_acc, n = 0.0, 0.0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(config.DEVICE), y.to(config.DEVICE)\n",
    "        logits = model(x)           # Forward pass\n",
    "        loss = criterion(logits, y) # Compute loss\n",
    "        metrics = calculate_metrics(logits, y.cpu())\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        total_loss += loss.item() * batch_size\n",
    "        total_acc += metrics[\"accuracy\"] * batch_size\n",
    "        n += batch_size\n",
    "    return total_loss / n, total_acc / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8dae69",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8c871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = build_dataloaders(trainset, testset, 16, config.SEED)\n",
    "\n",
    "# Training loop\n",
    "best_val_acc = 0.0\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(config.EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(config.DEVICE), y.to(config.DEVICE)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        predicted = torch.argmax(logits, dim=1).cpu()\n",
    "        total_train += y.size(0)\n",
    "        correct_train += (predicted == y.cpu()).sum().item()\n",
    "    \n",
    "    train_losses.append(train_loss / len(train_loader))\n",
    "    train_accuracies.append(correct_train / total_train)\n",
    "\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "    print(f\"Epoch {epoch+1}/{config.EPOCHS} - Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_epoch = epoch + 1\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "print(f\"\\nBest validation accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f77139",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3867a777",
   "metadata": {},
   "source": [
    "## Genetic Algorithm Hyperparameter Optimization (DEAP)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf99287",
   "metadata": {},
   "outputs": [],
   "source": [
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", dict, fitness=creator.FitnessMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917cc87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "toolbox = base.Toolbox()\n",
    "\n",
    "# Attribute generator\n",
    "toolbox.register(\"attr_batch_size\", random.choice, [16, 32, 64, 128, 256, 512, 1024])\n",
    "toolbox.register(\"attr_hidden_size\", random.choice, [16, 32, 64, 128, 256])\n",
    "toolbox.register(\"attr_hidden_layers\", random.randint, 1, 5)\n",
    "toolbox.register(\"attr_activation_function\", random.choice, [nn.Tanh, nn.ReLU, nn.LeakyReLU])\n",
    "toolbox.register(\"attr_dropout_p\", random.uniform, 0.0, 0.3)\n",
    "toolbox.register(\"attr_optimizer\", random.choice, [optim.SGD, optim.Adam, optim.RMSprop])\n",
    "toolbox.register(\"attr_lr\", random.choice, [0.001, 0.01, 0.005, 0.05, 0.003, 0.03])\n",
    "toolbox.register(\"attr_weight_decay\", random.choice, [0.0, 1e-5, 1e-4, 1e-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b852c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.RMSprop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee8cd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_individual():\n",
    "    layers = toolbox.attr_hidden_layers()\n",
    "    return {\n",
    "        \"batch_size\": toolbox.attr_batch_size(),\n",
    "        \"hidden_size\": toolbox.attr_hidden_size(),\n",
    "        \"hidden_layers\": layers,\n",
    "        \"activation_functions\": [toolbox.attr_activation_function() for _ in range(layers)],\n",
    "        \"dropout_p\": toolbox.attr_dropout_p(),\n",
    "        \"optimizer\": toolbox.attr_optimizer(),\n",
    "        \"lr\": toolbox.attr_lr(),\n",
    "        \"weight_decay\": toolbox.attr_weight_decay()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc6f708",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_individual()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d8544a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure initializers\n",
    "toolbox.register(\"individual\", tools.initIterate, creator.Individual, create_individual)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a672d0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_mlp(individual: creator.Individual) -> Tuple[float]:\n",
    "    hidden_size = individual[\"hidden_size\"]\n",
    "    hidden_layers = individual[\"hidden_layers\"]\n",
    "    activation_functions = individual[\"activation_functions\"]\n",
    "    dropout_p = individual[\"dropout_p\"]\n",
    "    batch_size = individual[\"batch_size\"]\n",
    "    optimizer = individual[\"optimizer\"]\n",
    "    lr = individual[\"lr\"]\n",
    "    weight_decay = individual[\"weight_decay\"]\n",
    "\n",
    "    # Instantiate model\n",
    "    model = MLP(\n",
    "        input_dim=config.INPUT_DIM,\n",
    "        hidden_dim=hidden_size,\n",
    "        output_dim=config.OUTPUT_DIM,\n",
    "        dropout_p=dropout_p,\n",
    "        hidden_layers=hidden_layers,\n",
    "        activation_functions=activation_functions\n",
    "    ).to(config.DEVICE)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optim = optimizer(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    train_loader, val_loader, _ = build_dataloaders(batch_size, config.SEED)\n",
    "\n",
    "    # Training loop\n",
    "    best_val_acc = 0.0\n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "    for epoch in range(config.EPOCHS):\n",
    "        model.train()\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(config.DEVICE), y.to(config.DEVICE)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "\n",
    "    return (best_val_acc,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59978df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate(individual: creator.Individual, indpb: float) -> Tuple[creator.Individual]:\n",
    "    if random.uniform(0.0, 1.0) < indpb:\n",
    "        individual[\"batch_size\"] = toolbox.attr_batch_size()\n",
    "    if random.uniform(0.0, 1.0) < indpb:\n",
    "        individual[\"hidden_size\"] = toolbox.attr_hidden_size()\n",
    "    if random.uniform(0.0, 1.0) < indpb:\n",
    "        individual[\"hidden_layers\"] = toolbox.attr_hidden_layers()\n",
    "    if random.uniform(0.0, 1.0) < indpb:\n",
    "        individual[\"dropout_p\"] = toolbox.attr_dropout_p()\n",
    "    if random.uniform(0.0, 1.0) < indpb:\n",
    "        individual[\"optimizer\"] = toolbox.attr_optimizer()\n",
    "    if random.uniform(0.0, 1.0) < indpb:\n",
    "        individual[\"lr\"] = toolbox.attr_lr()\n",
    "    if random.uniform(0.0, 1.0) < indpb:\n",
    "        individual[\"weight_decay\"] = toolbox.attr_weight_decay()\n",
    "\n",
    "    for af in range(len(individual[\"activation_functions\"])):\n",
    "        if random.uniform(0.0, 1.0) < indpb:\n",
    "            individual[\"activation_functions\"][af] = toolbox.attr_activation_function()\n",
    "\n",
    "    while individual[\"hidden_layers\"] < len(individual[\"activation_functions\"]):\n",
    "        individual[\"activation_functions\"].pop()\n",
    "\n",
    "    while individual[\"hidden_layers\"] > len(individual[\"activation_functions\"]):\n",
    "        individual[\"activation_functions\"].append(toolbox.attr_activation_function())\n",
    "\n",
    "    return (individual,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1495a1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossover(ind1: creator.Individual, ind2: creator.Individual) -> Tuple[creator.Individual]:\n",
    "    attributes = list(ind1.keys())\n",
    "    attribute = random.choice(attributes)\n",
    "    ind1[attribute], ind2[attribute] = ind2[attribute], ind1[attribute]\n",
    "\n",
    "    # Normalize activation_functions to match hidden_layers for both inds\n",
    "    for ind in (ind1, ind2):\n",
    "        while ind[\"hidden_layers\"] < len(ind[\"activation_functions\"]):\n",
    "            ind[\"activation_functions\"].pop()\n",
    "        while ind[\"hidden_layers\"] > len(ind[\"activation_functions\"]):\n",
    "            ind[\"activation_functions\"].append(toolbox.attr_activation_function())\n",
    "\n",
    "    return ind1, ind2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7013d709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select(population: List[creator.Individual], k: int, tournsize: int) -> List[creator.Individual]:\n",
    "    selected = []\n",
    "    for _ in range(k):\n",
    "        aspirants = random.sample(population, tournsize)\n",
    "        best = max(aspirants, key=lambda ind: ind.fitness.values)\n",
    "        selected.append(best)\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcbfdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operators\n",
    "toolbox.register(\"evaluate\", eval_mlp)\n",
    "toolbox.register(\"mate\", crossover)\n",
    "toolbox.register(\"mutate\", mutate, indpb=config.MUTATION_PROB)\n",
    "toolbox.register(\"select\", select, tournsize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c3f110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GA() -> Tuple[List[creator.Individual], tools.Logbook]:\n",
    "    pool = ThreadPool(multiprocessing.cpu_count())\n",
    "    toolbox.register(\"map\", pool.map)\n",
    "    \n",
    "    population = toolbox.population(n=config.POPULATION_SIZE)\n",
    "    hof = tools.HallOfFame(1)\n",
    "\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"std\", np.std)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "    population, logbook = algorithms.eaSimple(\n",
    "        population,\n",
    "        toolbox,\n",
    "        cxpb=config.CROSSOVER_PROB,\n",
    "        mutpb=config.MUTATION_PROB,\n",
    "        ngen=config.NUMBER_OF_GENERATIONS,\n",
    "        stats=stats,\n",
    "        halloffame=hof,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    return population, logbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b200baa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    pop, log = GA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bd266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_individual = tools.selBest(pop, 1)[0]\n",
    "best_individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8402a39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_individual.fitness.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43532ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = log.select(\"gen\")\n",
    "fit_avg = log.select(\"avg\")\n",
    "fit_max = log.select(\"max\")\n",
    "\n",
    "plt.plot(gen, fit_avg, label=\"Average Fitness\")\n",
    "plt.plot(gen, fit_max, label=\"Max Fitness\")\n",
    "plt.xlabel(\"Generation\")\n",
    "plt.ylabel(\"Fitness\")\n",
    "plt.title(\"Fitness over Generations\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39601f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_file = \"../results/final_pop.pkl\"\n",
    "logbook_file = \"../results/logbook.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882be09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "with open(pop_file, \"wb\") as f:\n",
    "    pickle.dump(pop, f)\n",
    "\n",
    "with open(logbook_file, \"wb\") as f:\n",
    "    pickle.dump(log, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e374483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "with open(pop_file, \"rb\") as f:\n",
    "    pop = pickle.load(f)\n",
    "\n",
    "with open(logbook_file, \"rb\") as f:\n",
    "    logbook = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05086de",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = best_individual[\"batch_size\"]\n",
    "hidden_size = best_individual[\"hidden_size\"]\n",
    "hidden_layers = best_individual[\"hidden_layers\"]\n",
    "activation_functions = best_individual[\"activation_functions\"]\n",
    "dropout_p = best_individual[\"dropout_p\"]\n",
    "optimizer = best_individual[\"optimizer\"]\n",
    "lr = best_individual[\"lr\"]\n",
    "weight_decay = best_individual[\"weight_decay\"]\n",
    "\n",
    "train_loader, val_loader, test_loader = build_dataloaders(batch_size, config.SEED)\n",
    "\n",
    "model = MLP(\n",
    "    input_dim=config.INPUT_DIM,\n",
    "    hidden_dim=hidden_size,\n",
    "    output_dim=config.OUTPUT_DIM,\n",
    "    dropout_p=dropout_p,\n",
    "    hidden_layers=hidden_layers,\n",
    "    activation_functions=activation_functions,\n",
    ").to(config.DEVICE)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optim = optimizer(model.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1a2074",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_acc = 0.0\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "history = {\"val_loss\": [], \"val_acc\": []}\n",
    "for epoch in range(config.EPOCHS):\n",
    "    model.train()\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(config.DEVICE), y.to(config.DEVICE)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{config.EPOCHS} - Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e721672",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "\n",
    "y_true = []\n",
    "y_prob = []\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x = x.to(config.DEVICE)\n",
    "        logits = model(x)\n",
    "        probs = torch.sigmoid(logits).squeeze(1).cpu().numpy()\n",
    "        y_prob.extend(probs.tolist())\n",
    "        y_true.extend(y.squeeze(1).numpy().tolist())\n",
    "\n",
    "y_true = np.array(y_true).astype(int)\n",
    "y_prob = np.array(y_prob)\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
    "try:\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "except ValueError:\n",
    "    auc = float(\"nan\")\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print({\n",
    "    \"test_loss\": round(test_loss, 4),\n",
    "    \"test_acc\": round(acc, 4),\n",
    "    \"precision\": round(prec, 4),\n",
    "    \"recall\": round(rec, 4),\n",
    "    \"f1\": round(f1, 4),\n",
    "    \"roc_auc\": round(auc, 4) if not np.isnan(auc) else auc,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a554ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef3333a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f\"ROC AUC = {auc:.3f}\")\n",
    "plt.plot([0, 1], [0, 1], \"r--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
